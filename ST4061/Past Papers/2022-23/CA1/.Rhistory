install.packages("caret")
library(MASS)
library(caret)
ca.train = read.csv("ca_train.csv",stringsAsFactors=TRUE)
ca.test = read.csv("ca_test.csv",stringsAsFactors=TRUE)
# 1
lda.o = lda(y~., data=ca.train)
lda.p = predict(lda.o, newdata=ca.test)$class
(ltb = confusionMatrix(lda.p, ca.test$y)$table)
# 2
qda.o = lda(y~., data=ca.train)
qda.p = predict(qda.o, newdata=ca.test)$class
(qtb = confusionMatrix(qda.p, ca.test$y)$table)
# 3
ltb[1, 1] / sum(ltb[, 1])
qtb[1, 1] / sum(qtb[, 1])
install.packages("caret")
library(MASS)
library(caret)
ca.train = read.csv("ca_train.csv",stringsAsFactors=TRUE)
ca.test = read.csv("ca_test.csv",stringsAsFactors=TRUE)
# 1
lda.o = lda(y~., data=ca.train)
lda.p = predict(lda.o, newdata=ca.test)$class
(ltb = confusionMatrix(lda.p, ca.test$y)$table)
# 2
qda.o = lda(y~., data=ca.train)
qda.p = predict(qda.o, newdata=ca.test)$class
(qtb = confusionMatrix(qda.p, ca.test$y)$table)
# 3
ltb[1, 1] / sum(ltb[, 1])
qtb[1, 1] / sum(qtb[, 1])
install.packages("caret")
install.packages("caret", dependencies = TRUE)
library(ISLR)
library(gbm)
install.packages(c("gbm", "ISLR"))
library(MASS)
library(caret)
ca.train = read.csv("ca_train.csv",stringsAsFactors=TRUE)
ca.test = read.csv("ca_test.csv",stringsAsFactors=TRUE)
# 1
lda.o = lda(y~., data=ca.train)
lda.p = predict(lda.o, newdata=ca.test)$class
(ltb = confusionMatrix(lda.p, ca.test$y)$table)
# 2
qda.o = lda(y~., data=ca.train)
qda.p = predict(qda.o, newdata=ca.test)$class
(qtb = confusionMatrix(qda.p, ca.test$y)$table)
# 3
ltb[1, 1] / sum(ltb[, 1])
qtb[1, 1] / sum(qtb[, 1])
install.packages("caret", dependencies = TRUE)
install.packages(c('callr', 'desc', 'pkgload', 'processx', 'waldo'))
install.packages("caret", dependencies = TRUE)
library(MASS)
library(caret)
ca.train = read.csv("ca_train.csv",stringsAsFactors=TRUE)
ca.test = read.csv("ca_test.csv",stringsAsFactors=TRUE)
# 1
lda.o = lda(y~., data=ca.train)
lda.p = predict(lda.o, newdata=ca.test)$class
(ltb = confusionMatrix(lda.p, ca.test$y)$table)
# 2
qda.o = lda(y~., data=ca.train)
qda.p = predict(qda.o, newdata=ca.test)$class
(qtb = confusionMatrix(qda.p, ca.test$y)$table)
# 3
ltb[1, 1] / sum(ltb[, 1])
qtb[1, 1] / sum(qtb[, 1])
install.packages(c(
"ggplot2", "lattice", "lazyeval", "recipes", "reshape2",
"ModelMetrics", "foreach", "iterators", "e1071"
))
R.version.string
library(MASS)
library(caret)
ca.train = read.csv("ca_train.csv",stringsAsFactors=TRUE)
ca.test = read.csv("ca_test.csv",stringsAsFactors=TRUE)
# 1
lda.o = lda(y~., data=ca.train)
lda.p = predict(lda.o, newdata=ca.test)$class
(ltb = confusionMatrix(lda.p, ca.test$y)$table)
# 2
qda.o = lda(y~., data=ca.train)
qda.p = predict(qda.o, newdata=ca.test)$class
(qtb = confusionMatrix(qda.p, ca.test$y)$table)
# 3
ltb[1, 1] / sum(ltb[, 1])
qtb[1, 1] / sum(qtb[, 1])
View(ca.test)
View(ca.test)
View(ca.train)
View(ca.train)
# Fit an LDA model using all predictors in the training data to predict 'y'
lda.o = lda(y ~ ., data = ca.train)  # . means "use all other variables as predictors"
# Use the fitted LDA model to predict class labels for the test data
lda.p = predict(lda.o, newdata = ca.test)$class  # Extract only the predicted class labels
# Generate and display the confusion matrix comparing predicted vs actual labels
(ltb = confusionMatrix(lda.p, ca.test$y)$table)  # Returns a cross-tabulation of predictions vs truth
ltb[1, 1]
ltb[, 1]
set.seed(4061)
library(ISLR)
library(gbm)
library(randomForest)
x.train = Khan$xtrain
x.test = Khan$xtest
y.train = as.factor(Khan$ytrain)
y.test = as.factor(Khan$ytest)
View(x.test)
View(x.test)
View(x.train)
View(x.train)
y.train
y.test
table(y.train)
table(y.test)
rfo = randomForest(x.train, y.train)
rfo
rfpr = predict(rfo, xtest, type = "prob")
(4061)
set.seed(4061)
rfpr = predict(rfo, x.test, type = "prob")
rfpr
rfp = predict(rfo, x.test)
rfp
y.train
y.test
table(y.test)
tb = table(rfp, y.test)
tb
table(y.test)
sum(diag(tb) / sum(tb))
is = which(rfo$importance > 0.4)
is = which(rfo$importance > 0.4)
rownames(rfo$importance)[is]
hist(rfo$importance[is])
barplot((rfo$importance[is,]), las=2)
hist(rfo$importance[is])
barplot((rfo$importance[is,]), las=2)
gb.out = gbm(y.train~., data = as.data.frame(x.train), distribution = "multinomial")
gb.fitted = predict(gb.out, n.trees = gb.out$n.trees)
gb.pred = predict(gb.out, as.data.frame(x.test), n.trees = gb.out$n.trees)
gbp = apply(gb.pred, 1, which.max)
tb = table(y.test, gbp)
tb
sum(diag(tb)) / sum(tb)
