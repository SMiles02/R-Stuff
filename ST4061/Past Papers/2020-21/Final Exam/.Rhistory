# (a) and (b)
dat = read.csv(file="dodgysales.csv", stringsAsFactors=TRUE)
View(dat)
dat
n = nrow(dat)
P = ncol(dat) - 1  # Number of predictors
# (c) i
# Min-max normalisation
minmax = function(x) (x - min(x)) / (max(x) - min(x))
dat.s = as.data.frame(lapply(dat, function(x) if(is.numeric(x)) minmax(x) else x))
View(dat.s)
View(dat.s)
summary(dat.s$Sales)
summary(dat.s$BudgOp)
table(dat.s$Training)
# Train-validation split
set.seed(6041)
i.train = sample(1:n, floor(.7 * n))
dat.s.train = dat.s[i.train,]
dat.s.validation = dat.s[-i.train,]
# (c) ii: Fit neural networks
library(nnet)
set.seed(6041)
nn3 = nnet(Sales ~ ., data = dat.s.train, size = 3, linout = TRUE)
nn8 = nnet(Sales ~ ., data = dat.s.train, size = 8, linout = TRUE)
mean(nn3$residuals^2)  # Training MSE for 3 neurons
mean(nn8$residuals^2)  # Training MSE for 8 neurons
View(dat.s.validation)
# (c) ii: Fit neural networks
library(neuralnet)
set.seed(6041)
fml <- as.formula(paste("Sales ~", paste(predictors, collapse = " + ")))
fml
nn3 <- neuralnet(Sales~., data = dat.s.train, hidden = 3, linear.output = TRUE)
library(neuralnet)
set.seed(4061)
n = nrow(iris)
dat = iris[sample(1:n), ] # shuffle initial dataset
NC = ncol(dat)
nno = neuralnet(Species~., data=dat, hidden=c(6,5))
plot(nno)
# (c) ii: Fit neural networks
library(neuralnet)
set.seed(6041)
predictors <- names(dat.s)[names(dat.s) != "Sales"]
fml <- as.formula(paste("Sales ~", paste(predictors, collapse = " + ")))
nn3 <- neuralnet(fml, data = dat.s.train, hidden = 3, linear.output = TRUE)
# (c) ii: Fit neural networks
library(nnet)
set.seed(6041)
dat.s.train = dat.s[i.train,]
dat.s.validation = dat.s[-i.train,]
nn3 = nnet(Sales ~ ., data = dat.s.train, size = 3, linout = TRUE)
nn8 = nnet(Sales ~ ., data = dat.s.train, size = 8, linout = TRUE)
mean(nn3$residuals^2)  # Training MSE for 3 neurons
mean(nn8$residuals^2)  # Training MSE for 8 neurons
# (c) iii: Validation MSEs
ytest = dat.s.validation$Sales
p3 = predict(nn3, dat.s.validation)
p8 = predict(nn8, dat.s.validation)
mean((p3 - ytest)^2)
mean((p8 - ytest)^2)
# (d) Gradient Boosting Model
library(gbm)
set.seed(6041)
gbmo = gbm(Sales ~ ., data = dat.train, n.trees = 100)
dat.train = dat[i.train,]
dat.validation = dat[-i.train,]
# (d) Gradient Boosting Model
library(gbm)
set.seed(6041)
gbmo = gbm(Sales ~ ., data = dat.train, n.trees = 100)
gbmo = gbm(dat$Sales ~ ., data = dat.train, n.trees = 100)
dat.train = dat[i.train,]
dat.validation = dat[-i.train,]
View(dat.train)
# (a) and (b)
dat = read.csv(file="dodgysales.csv", stringsAsFactors=TRUE)
n = nrow(dat)
P = ncol(dat) - 1  # Number of predictors
dat.train = dat[i.train,]
dat.validation = dat[-i.train,]
gbmo = gbm(Sales ~ ., data = dat.train, n.trees = 100)
gbmp = predict(gbmo, newdata = dat.validation, n.trees = 100)
mean((gbmo$fit - dat.train$Sales)^2)       # Training MSE
mean((gbmp - dat.validation$Sales)^2)      # Validation MSE
glmo = glm(Sales ~ ., data = dat.train)
glmp = predict(glmo, dat.validation)
mean((glmo$fitted.values - dat.train$Sales)^2)
mean((glmp - dat.validation$Sales)^2)
# (f) Ridge Regression
library(glmnet)
x = model.matrix(Sales ~ ., dat.train)[, -1]
y = dat.train$Sales
x.test = model.matrix(Sales ~ ., dat.validation)[, -1]
y.test = dat.validation$Sales
set.seed(6041)
ridge.fit = glmnet(x, y, alpha = 0)
ridgep = predict(ridge.fit, s = 0.01, newx = x.test)
mean((predict(ridge.fit, s = 0.01, newx = x) - y)^2)  # Training
mean((ridgep - y.test)^2)  # Validation
library(mlbench)
data(BreastCancer)
dat = na.omit(BreastCancer)
dat$Id = NULL
set.seed(4061)
i.train = sample(1:nrow(dat), 600, replace=FALSE)
dat.train = dat[i.train,]
dat.validation = dat[-i.train,]
library(caret)
# Fit random forest using caret with 10-fold CV
ctrl = trainControl(method = "cv", number = 10)
rf.model = train(
Class ~ ., data = dat.train,
method = "rf",
trControl = ctrl
)
# Output the model to get number of variables used at each split
rf.model
rf.pred = predict(rf.model, newdata = dat.validation)
rf.conf = confusionMatrix(rf.pred, dat.validation$Class)
rf.conf$overall['Accuracy']
svm.linear = train(
Class ~ ., data = dat.train,
method = "svmLinear",
trControl = ctrl
)
svm.linear = train(
Class ~ ., data = dat.train,
method = "svmLinear",
trControl = ctrl
)
svm.linear.pred = predict(svm.linear, newdata = dat.validation)
confusionMatrix(svm.linear.pred, dat.validation$Class)$overall['Accuracy']
svm.radial = train(
Class ~ ., data = dat.train,
method = "svmRadial",
trControl = ctrl
)
svm.radial.pred = predict(svm.radial, newdata = dat.validation)
confusionMatrix(svm.radial.pred, dat.validation$Class)$overall['Accuracy']
# (b)
svm.linear = train(
Class ~ ., data = dat.train,
method = "svmLinear",
trControl = ctrl
)
svm.linear.pred = predict(svm.linear, newdata = dat.validation)
confusionMatrix(svm.linear.pred, dat.validation$Class)$overall['Accuracy']
# (c)
svm.radial = train(
Class ~ ., data = dat.train,
method = "svmRadial",
trControl = ctrl
)
svm.radial.pred = predict(svm.radial, newdata = dat.validation)
confusionMatrix(svm.radial.pred, dat.validation$Class)$overall['Accuracy']
# (f)
importance = varImp(rf.model)
plot(importance)
print(importance)
importance2 = varImp(rf.model)
importance2 = varImp(svm.linear)
plot(importance2)
print(importance2)
importance3 = varImp(svm.radial)
plot(importance3)
print(importance3)
View(importance2)
View(importance2)
View(importance3)
View(importance3)
